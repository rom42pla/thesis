[04/24 16:47:44] detectron2 INFO: Rank of current process: 0. World size: 1
[04/24 16:47:45] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.4 (default, Apr  9 2021, 16:34:09) [GCC 7.3.0]
numpy                   1.20.2
detectron2              0.4 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.2
detectron2 arch flags   6.1
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.8.1+cu111 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0                   GeForce GTX 1050 (arch=6.1)
CUDA_HOME               /usr/local/cuda
Pillow                  8.2.0
torchvision             0.9.1+cu111 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20210415
iopath                  0.1.7
cv2                     4.5.1
----------------------  --------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 16:47:45] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/24 16:47:45] detectron2 INFO: Contents of args.config_file=configs/detr.yaml:
MODEL:
  META_ARCHITECTURE: "Detr"
#  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: False
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  DETR:
    GIOU_WEIGHT: 2.0
    L1_WEIGHT: 5.0
    NUM_OBJECT_QUERIES: 100
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 64
  BASE_LR: 0.0001
  STEPS: (369600,)
  MAX_ITER: 554400
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
INPUT:
  MIN_SIZE_TRAIN: (480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800)
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
TEST:
  EVAL_PERIOD: 4000
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2

[04/24 16:47:45] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: false
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val
  TRAIN:
  - coco_2017_train
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: true
    SIZE:
    - 384
    - 600
    TYPE: absolute_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 480
  - 512
  - 544
  - 576
  - 608
  - 640
  - 672
  - 704
  - 736
  - 768
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  DETR:
    DEC_LAYERS: 6
    DEEP_SUPERVISION: true
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.1
    ENC_LAYERS: 6
    FROZEN_WEIGHTS: ''
    GIOU_WEIGHT: 2.0
    HIDDEN_DIM: 256
    L1_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_CLASSES: 80
    NUM_OBJECT_QUERIES: 100
    PRE_NORM: false
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: Detr
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: ''
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 64
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 554400
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 369600
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 4000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/24 16:47:45] detectron2 INFO: Full config saved to ./output/config.yaml
[04/24 16:47:45] d2.utils.env INFO: Using a generated random seed 45498622
[04/24 16:47:47] d2.engine.defaults INFO: Model:
Detr(
  (detr): DETR(
    (transformer): Transformer(
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (class_embed): Linear(in_features=256, out_features=81, bias=True)
    (bbox_embed): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embed): Embedding(100, 256)
    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionEmbeddingSine()
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[04/24 16:48:00] d2.data.datasets.coco INFO: Loading datasets/coco/annotations/instances_train2017.json takes 12.27 seconds.
[04/24 16:48:00] d2.data.datasets.coco INFO: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[04/24 16:48:11] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |[0m
[04/24 16:48:11] d2.data.build INFO: Using training sampler TrainingSampler
[04/24 16:48:11] d2.data.common INFO: Serializing 118287 elements to byte tensors and concatenating them all ...
[04/24 16:48:13] d2.data.common INFO: Serialized dataset takes 451.34 MiB
[04/24 16:48:15] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 16:48:15] d2.engine.train_loop INFO: Starting training from iteration 0
[04/24 16:48:16] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 495, in run_step
    self._trainer.run_step()
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/git_repos/few-shot-object-detection/d2/detr/detr.py", line 177, in forward
    output = self.detr(images)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/git_repos/few-shot-object-detection/models/detr.py", line 61, in forward
    features, pos = self.backbone(samples)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/git_repos/few-shot-object-detection/models/backbone.py", line 101, in forward
    xs = self[0](tensor_list)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/git_repos/few-shot-object-detection/d2/detr/detr.py", line 42, in forward
    features = self.backbone(images.tensor)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/modeling/backbone/resnet.py", line 445, in forward
    x = self.stem(x)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/modeling/backbone/resnet.py", line 356, in forward
    x = self.conv1(x)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2/layers/wrappers.py", line 84, in forward
    x = F.conv2d(
RuntimeError: CUDA out of memory. Tried to allocate 4.07 GiB (GPU 0; 3.95 GiB total capacity; 940.81 MiB already allocated; 1.95 GiB free; 960.00 MiB reserved in total by PyTorch)
[04/24 16:48:16] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[04/24 16:48:16] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 1330M
[04/24 16:53:07] detectron2 INFO: Rank of current process: 0. World size: 1
[04/24 16:53:08] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.4 (default, Apr  9 2021, 16:34:09) [GCC 7.3.0]
numpy                   1.20.2
detectron2              0.4 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/detectron2
Compiler                GCC 9.3
CUDA compiler           CUDA 11.2
detectron2 arch flags   6.1
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.8.1+cu111 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           True
GPU 0                   GeForce GTX 1050 (arch=6.1)
CUDA_HOME               /usr/local/cuda
Pillow                  8.2.0
torchvision             0.9.1+cu111 @/home/rom42pla/anaconda3/envs/fsd/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20210415
iopath                  0.1.7
cv2                     4.5.1
----------------------  --------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 16:53:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/24 16:53:08] detectron2 INFO: Contents of args.config_file=configs/detr.yaml:
MODEL:
  META_ARCHITECTURE: "Detr"
#  WEIGHTS: "detectron2://ImageNetPretrained/torchvision/R-50.pkl"
  WEIGHTS: "checkpoints/detr-r101-dc5.pth"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  MASK_ON: False
  RESNETS:
    DEPTH: 50
    STRIDE_IN_1X1: False
    OUT_FEATURES: ["res2", "res3", "res4", "res5"]
  DETR:
    GIOU_WEIGHT: 2.0
    L1_WEIGHT: 5.0
    NUM_OBJECT_QUERIES: 100
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 64
  BASE_LR: 0.0001
  STEPS: (369600,)
  MAX_ITER: 554400
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.0001
  OPTIMIZER: "ADAMW"
  BACKBONE_MULTIPLIER: 0.1
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
INPUT:
  MIN_SIZE_TRAIN: (480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800)
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  FORMAT: "RGB"
TEST:
  EVAL_PERIOD: 4000
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: False
  NUM_WORKERS: 4
VERSION: 2

[04/24 16:53:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: false
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val
  TRAIN:
  - coco_2017_train
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: true
    SIZE:
    - 384
    - 600
    TYPE: absolute_range
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 480
  - 512
  - 544
  - 576
  - 608
  - 640
  - 672
  - 704
  - 736
  - 768
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  DETR:
    DEC_LAYERS: 6
    DEEP_SUPERVISION: true
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.1
    ENC_LAYERS: 6
    FROZEN_WEIGHTS: ''
    GIOU_WEIGHT: 2.0
    HIDDEN_DIM: 256
    L1_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_CLASSES: 80
    NUM_OBJECT_QUERIES: 100
    PRE_NORM: false
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: Detr
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: checkpoints/detr-r101-dc5.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 64
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 554400
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 369600
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 4000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/24 16:53:08] detectron2 INFO: Full config saved to ./output/config.yaml
[04/24 16:53:08] d2.utils.env INFO: Using a generated random seed 8202296
[04/24 16:53:10] d2.engine.defaults INFO: Model:
Detr(
  (detr): DETR(
    (transformer): Transformer(
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (class_embed): Linear(in_features=256, out_features=81, bias=True)
    (bbox_embed): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embed): Embedding(100, 256)
    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionEmbeddingSine()
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[04/24 16:53:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/detr-r101-dc5.pth ...
[04/24 16:53:10] fvcore.common.checkpoint INFO: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.stem.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.stem.conv1.weight[0m
[34mdetr.bbox_embed.layers.0.{bias, weight}[0m
[34mdetr.bbox_embed.layers.1.{bias, weight}[0m
[34mdetr.bbox_embed.layers.2.{bias, weight}[0m
[34mdetr.class_embed.{bias, weight}[0m
[34mdetr.input_proj.{bias, weight}[0m
[34mdetr.query_embed.weight[0m
[34mdetr.transformer.decoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.norm.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[04/24 16:53:10] fvcore.common.checkpoint INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mtransformer.encoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.0.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.0.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.0.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.0.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.0.norm2.{bias, weight}[0m
  [35mtransformer.encoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.1.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.1.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.1.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.1.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.1.norm2.{bias, weight}[0m
  [35mtransformer.encoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.2.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.2.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.2.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.2.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.2.norm2.{bias, weight}[0m
  [35mtransformer.encoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.3.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.3.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.3.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.3.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.3.norm2.{bias, weight}[0m
  [35mtransformer.encoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.4.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.4.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.4.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.4.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.4.norm2.{bias, weight}[0m
  [35mtransformer.encoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.encoder.layers.5.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.encoder.layers.5.linear1.{bias, weight}[0m
  [35mtransformer.encoder.layers.5.linear2.{bias, weight}[0m
  [35mtransformer.encoder.layers.5.norm1.{bias, weight}[0m
  [35mtransformer.encoder.layers.5.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.0.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.0.norm3.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.1.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.1.norm3.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.2.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.2.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.2.norm3.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.3.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.3.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.3.norm3.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.4.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.4.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.4.norm3.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.5.self_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
  [35mtransformer.decoder.layers.5.multihead_attn.out_proj.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.linear1.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.linear2.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.norm1.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.norm2.{bias, weight}[0m
  [35mtransformer.decoder.layers.5.norm3.{bias, weight}[0m
  [35mtransformer.decoder.norm.{bias, weight}[0m
  [35mclass_embed.{bias, weight}[0m
  [35mbbox_embed.layers.0.{bias, weight}[0m
  [35mbbox_embed.layers.1.{bias, weight}[0m
  [35mbbox_embed.layers.2.{bias, weight}[0m
  [35mquery_embed.weight[0m
  [35minput_proj.{bias, weight}[0m
  [35mbackbone.0.body.conv1.weight[0m
  [35mbackbone.0.body.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.0.conv1.weight[0m
  [35mbackbone.0.body.layer1.0.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.0.conv2.weight[0m
  [35mbackbone.0.body.layer1.0.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.0.conv3.weight[0m
  [35mbackbone.0.body.layer1.0.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.0.downsample.0.weight[0m
  [35mbackbone.0.body.layer1.0.downsample.1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.1.conv1.weight[0m
  [35mbackbone.0.body.layer1.1.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.1.conv2.weight[0m
  [35mbackbone.0.body.layer1.1.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.1.conv3.weight[0m
  [35mbackbone.0.body.layer1.1.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.2.conv1.weight[0m
  [35mbackbone.0.body.layer1.2.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.2.conv2.weight[0m
  [35mbackbone.0.body.layer1.2.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer1.2.conv3.weight[0m
  [35mbackbone.0.body.layer1.2.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.0.conv1.weight[0m
  [35mbackbone.0.body.layer2.0.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.0.conv2.weight[0m
  [35mbackbone.0.body.layer2.0.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.0.conv3.weight[0m
  [35mbackbone.0.body.layer2.0.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.0.downsample.0.weight[0m
  [35mbackbone.0.body.layer2.0.downsample.1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.1.conv1.weight[0m
  [35mbackbone.0.body.layer2.1.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.1.conv2.weight[0m
  [35mbackbone.0.body.layer2.1.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.1.conv3.weight[0m
  [35mbackbone.0.body.layer2.1.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.2.conv1.weight[0m
  [35mbackbone.0.body.layer2.2.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.2.conv2.weight[0m
  [35mbackbone.0.body.layer2.2.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.2.conv3.weight[0m
  [35mbackbone.0.body.layer2.2.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.3.conv1.weight[0m
  [35mbackbone.0.body.layer2.3.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.3.conv2.weight[0m
  [35mbackbone.0.body.layer2.3.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer2.3.conv3.weight[0m
  [35mbackbone.0.body.layer2.3.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.0.conv1.weight[0m
  [35mbackbone.0.body.layer3.0.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.0.conv2.weight[0m
  [35mbackbone.0.body.layer3.0.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.0.conv3.weight[0m
  [35mbackbone.0.body.layer3.0.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.0.downsample.0.weight[0m
  [35mbackbone.0.body.layer3.0.downsample.1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.1.conv1.weight[0m
  [35mbackbone.0.body.layer3.1.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.1.conv2.weight[0m
  [35mbackbone.0.body.layer3.1.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.1.conv3.weight[0m
  [35mbackbone.0.body.layer3.1.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.2.conv1.weight[0m
  [35mbackbone.0.body.layer3.2.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.2.conv2.weight[0m
  [35mbackbone.0.body.layer3.2.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.2.conv3.weight[0m
  [35mbackbone.0.body.layer3.2.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.3.conv1.weight[0m
  [35mbackbone.0.body.layer3.3.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.3.conv2.weight[0m
  [35mbackbone.0.body.layer3.3.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.3.conv3.weight[0m
  [35mbackbone.0.body.layer3.3.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.4.conv1.weight[0m
  [35mbackbone.0.body.layer3.4.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.4.conv2.weight[0m
  [35mbackbone.0.body.layer3.4.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.4.conv3.weight[0m
  [35mbackbone.0.body.layer3.4.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.5.conv1.weight[0m
  [35mbackbone.0.body.layer3.5.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.5.conv2.weight[0m
  [35mbackbone.0.body.layer3.5.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.5.conv3.weight[0m
  [35mbackbone.0.body.layer3.5.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.6.conv1.weight[0m
  [35mbackbone.0.body.layer3.6.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.6.conv2.weight[0m
  [35mbackbone.0.body.layer3.6.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.6.conv3.weight[0m
  [35mbackbone.0.body.layer3.6.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.7.conv1.weight[0m
  [35mbackbone.0.body.layer3.7.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.7.conv2.weight[0m
  [35mbackbone.0.body.layer3.7.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.7.conv3.weight[0m
  [35mbackbone.0.body.layer3.7.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.8.conv1.weight[0m
  [35mbackbone.0.body.layer3.8.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.8.conv2.weight[0m
  [35mbackbone.0.body.layer3.8.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.8.conv3.weight[0m
  [35mbackbone.0.body.layer3.8.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.9.conv1.weight[0m
  [35mbackbone.0.body.layer3.9.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.9.conv2.weight[0m
  [35mbackbone.0.body.layer3.9.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.9.conv3.weight[0m
  [35mbackbone.0.body.layer3.9.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.10.conv1.weight[0m
  [35mbackbone.0.body.layer3.10.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.10.conv2.weight[0m
  [35mbackbone.0.body.layer3.10.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.10.conv3.weight[0m
  [35mbackbone.0.body.layer3.10.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.11.conv1.weight[0m
  [35mbackbone.0.body.layer3.11.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.11.conv2.weight[0m
  [35mbackbone.0.body.layer3.11.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.11.conv3.weight[0m
  [35mbackbone.0.body.layer3.11.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.12.conv1.weight[0m
  [35mbackbone.0.body.layer3.12.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.12.conv2.weight[0m
  [35mbackbone.0.body.layer3.12.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.12.conv3.weight[0m
  [35mbackbone.0.body.layer3.12.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.13.conv1.weight[0m
  [35mbackbone.0.body.layer3.13.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.13.conv2.weight[0m
  [35mbackbone.0.body.layer3.13.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.13.conv3.weight[0m
  [35mbackbone.0.body.layer3.13.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.14.conv1.weight[0m
  [35mbackbone.0.body.layer3.14.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.14.conv2.weight[0m
  [35mbackbone.0.body.layer3.14.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.14.conv3.weight[0m
  [35mbackbone.0.body.layer3.14.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.15.conv1.weight[0m
  [35mbackbone.0.body.layer3.15.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.15.conv2.weight[0m
  [35mbackbone.0.body.layer3.15.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.15.conv3.weight[0m
  [35mbackbone.0.body.layer3.15.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.16.conv1.weight[0m
  [35mbackbone.0.body.layer3.16.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.16.conv2.weight[0m
  [35mbackbone.0.body.layer3.16.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.16.conv3.weight[0m
  [35mbackbone.0.body.layer3.16.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.17.conv1.weight[0m
  [35mbackbone.0.body.layer3.17.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.17.conv2.weight[0m
  [35mbackbone.0.body.layer3.17.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.17.conv3.weight[0m
  [35mbackbone.0.body.layer3.17.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.18.conv1.weight[0m
  [35mbackbone.0.body.layer3.18.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.18.conv2.weight[0m
  [35mbackbone.0.body.layer3.18.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.18.conv3.weight[0m
  [35mbackbone.0.body.layer3.18.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.19.conv1.weight[0m
  [35mbackbone.0.body.layer3.19.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.19.conv2.weight[0m
  [35mbackbone.0.body.layer3.19.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.19.conv3.weight[0m
  [35mbackbone.0.body.layer3.19.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.20.conv1.weight[0m
  [35mbackbone.0.body.layer3.20.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.20.conv2.weight[0m
  [35mbackbone.0.body.layer3.20.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.20.conv3.weight[0m
  [35mbackbone.0.body.layer3.20.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.21.conv1.weight[0m
  [35mbackbone.0.body.layer3.21.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.21.conv2.weight[0m
  [35mbackbone.0.body.layer3.21.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.21.conv3.weight[0m
  [35mbackbone.0.body.layer3.21.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.22.conv1.weight[0m
  [35mbackbone.0.body.layer3.22.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.22.conv2.weight[0m
  [35mbackbone.0.body.layer3.22.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer3.22.conv3.weight[0m
  [35mbackbone.0.body.layer3.22.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.0.conv1.weight[0m
  [35mbackbone.0.body.layer4.0.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.0.conv2.weight[0m
  [35mbackbone.0.body.layer4.0.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.0.conv3.weight[0m
  [35mbackbone.0.body.layer4.0.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.0.downsample.0.weight[0m
  [35mbackbone.0.body.layer4.0.downsample.1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.1.conv1.weight[0m
  [35mbackbone.0.body.layer4.1.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.1.conv2.weight[0m
  [35mbackbone.0.body.layer4.1.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.1.conv3.weight[0m
  [35mbackbone.0.body.layer4.1.bn3.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.2.conv1.weight[0m
  [35mbackbone.0.body.layer4.2.bn1.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.2.conv2.weight[0m
  [35mbackbone.0.body.layer4.2.bn2.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.0.body.layer4.2.conv3.weight[0m
  [35mbackbone.0.body.layer4.2.bn3.{bias, running_mean, running_var, weight}[0m
[04/24 16:53:10] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[04/24 16:53:11] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[04/24 16:53:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/24 16:53:11] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[04/24 16:53:11] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[04/24 16:53:11] d2.evaluation.coco_evaluation WARNING: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[04/24 16:53:11] d2.evaluation.evaluator INFO: Start inference on 5000 images
[04/24 16:53:14] d2.evaluation.evaluator INFO: Inference done 11/5000. 0.1980 s / img. ETA=0:16:31
[04/24 16:53:19] d2.evaluation.evaluator INFO: Inference done 36/5000. 0.2015 s / img. ETA=0:16:45
[04/24 16:53:24] d2.evaluation.evaluator INFO: Inference done 61/5000. 0.2026 s / img. ETA=0:16:46
[04/24 16:53:30] d2.evaluation.evaluator INFO: Inference done 87/5000. 0.2004 s / img. ETA=0:16:30
[04/24 16:53:35] d2.evaluation.evaluator INFO: Inference done 114/5000. 0.1979 s / img. ETA=0:16:12
[04/24 16:53:40] d2.evaluation.evaluator INFO: Inference done 139/5000. 0.1985 s / img. ETA=0:16:10
[04/24 16:53:45] d2.evaluation.evaluator INFO: Inference done 164/5000. 0.1992 s / img. ETA=0:16:08
[04/24 16:53:50] d2.evaluation.evaluator INFO: Inference done 190/5000. 0.1988 s / img. ETA=0:16:01
[04/24 16:53:55] d2.evaluation.evaluator INFO: Inference done 216/5000. 0.1981 s / img. ETA=0:15:53
[04/24 16:54:00] d2.evaluation.evaluator INFO: Inference done 242/5000. 0.1979 s / img. ETA=0:15:47
[04/24 16:54:05] d2.evaluation.evaluator INFO: Inference done 268/5000. 0.1980 s / img. ETA=0:15:42
[04/24 16:54:11] d2.evaluation.evaluator INFO: Inference done 294/5000. 0.1978 s / img. ETA=0:15:36
[04/24 16:54:16] d2.evaluation.evaluator INFO: Inference done 320/5000. 0.1977 s / img. ETA=0:15:30
[04/24 16:54:21] d2.evaluation.evaluator INFO: Inference done 345/5000. 0.1979 s / img. ETA=0:15:26
[04/24 16:54:26] d2.evaluation.evaluator INFO: Inference done 370/5000. 0.1983 s / img. ETA=0:15:23
